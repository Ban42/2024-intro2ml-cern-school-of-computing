{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a25cdb06",
   "metadata": {},
   "source": [
    "# Exploring a reweighting task\n",
    "\n",
    "The following functions produce a dataset which can be used to illustrate the use of attention. In this notebook, we focus on exploring the data set with a standard convnet. The dataset exhibits 2 triangles and 2 boxes/rectangles on a 1D line. Think of this example being very similar to measuring the deposited energy xray radiation and a photon beam when traversing matter. The xrays deposit their energy continuously with some attentuation upto very high depth into the solid state object. Particles (like protons) exhibit a behavior called a Bragg peak, i.e. at a specific depth almost all of the dose is deposited and the beam does not traverse further.\n",
    "\n",
    "See also:\n",
    "![https://en.wikipedia.org/wiki/Bragg_peak](https://en.wikipedia.org/wiki/Bragg_peak#/media/File:BraggPeak-en.svg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f2ff26",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from fleuret_data import generate_sequences\n",
    "\n",
    "drng = np.random.default_rng(43)  # set the RNG seed for reproducible runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20354814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and test set\n",
    "train_input, train_targets, train_tr, train_bx = generate_sequences(\n",
    "    15000, seq_length=128, rng=drng\n",
    ")\n",
    "test_input, test_targets, test_tr, test_bx = generate_sequences(\n",
    "    1000, seq_length=128, rng=drng\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1fd4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input.shape, test_targets.shape, test_tr.shape, test_bx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7246df4d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "\n",
    "ax[0].plot(\n",
    "    np.arange(test_input[0].shape[-1]) + 0.5,\n",
    "    test_input[0].squeeze(),\n",
    "    color=\"blue\",\n",
    "    label=\"input\",\n",
    ")\n",
    "ax[0].set_title(\"input\")\n",
    "ax[1].plot(\n",
    "    np.arange(test_targets[0].shape[-1]) + 0.5,\n",
    "    test_targets[0].squeeze(),\n",
    "    color=\"red\",\n",
    "    label=\"target\",\n",
    ")\n",
    "ax[1].set_title(\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4949af4",
   "metadata": {},
   "source": [
    "You see two kinds of \"objects\" in the signal above: two box-like structures and two triangle-like structure. We define a **regression task** which is meant to equalize the height of the boxes (new height should be the average height of the two input boxes) and the height of the triangles (new height of the triangles should be the mean of the two input triangles)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ba499",
   "metadata": {},
   "source": [
    "## Convolutional Network\n",
    "\n",
    "In the following, we like to create a regression model using convolutions only, which tries to accomplish the task above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90e9c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to tell keras to use the torch backend\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d11341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now import keras\n",
    "import keras\n",
    "# set the seeds to make the notebook reproducible\n",
    "np.random.seed(41)\n",
    "keras.utils.set_random_seed(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877f0d03",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# normalize the signal, zscale if required\n",
    "x_min, x_max = train_input.min(), train_input.max()\n",
    "x_ = (train_input - x_min) / (x_max - x_min)\n",
    "\n",
    "y_min, y_max = train_targets.min(), train_targets.max()\n",
    "y_ = (train_targets - y_min) / (y_max - y_min)\n",
    "\n",
    "x_test_ = (test_input - x_min) / (x_max - x_min)\n",
    "y_test_ = (test_targets - y_min) / (y_max - y_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39145f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_.shape, y_.shape, x_test_.shape, y_test_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00713788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras being shipped with tensorflow is unable to digest this data\n",
    "# because it does we have to reshape it\n",
    "x = np.swapaxes(x_, -2, -1)\n",
    "y = np.swapaxes(y_, -2, -1)\n",
    "\n",
    "x_test = np.swapaxes(x_test_, -2, -1)\n",
    "y_test = np.swapaxes(y_test_, -2, -1)\n",
    "\n",
    "x.shape, y.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb993419",
   "metadata": {},
   "source": [
    "# creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43540e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_fcn(inshape=x.shape[-2:], channels=64, ksize=5):\n",
    "    \"a fully convolutional network (fcn) to regress the signal\"\n",
    "\n",
    "    inputs = keras.layers.Input(shape=inshape)\n",
    "    x = keras.layers.Conv1D(\n",
    "        channels, ksize, strides=1, padding=\"same\", activation=\"relu\"\n",
    "    )(inputs)\n",
    "    x = keras.layers.Conv1D(\n",
    "        channels, ksize, strides=1, padding=\"same\", activation=\"relu\"\n",
    "    )(x)\n",
    "    x = keras.layers.Conv1D(\n",
    "        channels, ksize, strides=1, padding=\"same\", activation=\"relu\"\n",
    "    )(x)\n",
    "    x = keras.layers.Conv1D(\n",
    "        channels, ksize, strides=1, padding=\"same\", activation=\"relu\"\n",
    "    )(x)\n",
    "    outputs = keras.layers.Conv1D(1, ksize, strides=1, padding=\"same\")(x)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=outputs, name=\"fcn-regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148e61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plainfcn = create_fcn(x.shape[1:])\n",
    "plainfcn.summary()  # a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9509f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plainfcn.compile(optimizer=\"adam\", loss=keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = plainfcn.fit(\n",
    "    x, y, validation_data=(x_test, y_test), batch_size=128, epochs=15, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d3cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def plot_history(history, metrics, draw_legend=True):\n",
    "    \"\"\"\n",
    "    Plot the training history\n",
    "\n",
    "    Args:\n",
    "        history (keras History object that is returned by model.fit())\n",
    "        metrics(str, list): Metric or a list of metrics to plot\n",
    "    \"\"\"\n",
    "    history_df = pd.DataFrame.from_dict(history.history)\n",
    "    sns.lineplot(data=history_df[metrics])\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"metric\")\n",
    "    if not draw_legend:\n",
    "        plt.legend().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bb13c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_history(history, [\"loss\", \"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e8167",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "pred5 = plainfcn.predict(x_test[:5, ...])\n",
    "pred5_ = np.swapaxes(pred5, -2, -1)  # useful for plotting\n",
    "pred5.shape, pred5_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58137516",
   "metadata": {},
   "outputs": [],
   "source": [
    "xaxis = np.arange(0, x_.shape[-1], 1)\n",
    "print(xaxis.shape)\n",
    "\n",
    "plt.plot(xaxis, y_test_[0:1, 0, ...].squeeze(), color=\"green\", label=\"label\")\n",
    "plt.plot(xaxis, pred5_[0:1, 0, ...].squeeze(), color=\"red\", label=\"prediction\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7774a77c",
   "metadata": {},
   "source": [
    "The above is not a great model, actually it doesn't work at all! But we expected no less as the loss didn't decrease any further than `0.0033`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f92bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_histories(histories, metrics, hist_labels=[], draw_legend=True):\n",
    "    \"\"\"\n",
    "    Plot the training progression of several histories\n",
    "\n",
    "    Args:\n",
    "        histories (list of keras History objects that was returned by model.fit())\n",
    "        metrics(str, list): Metric or a list of metrics to plot\n",
    "        hist_labels(list): list of strings describing each history\n",
    "    \"\"\"\n",
    "    assert len(histories) == len(hist_labels)\n",
    "\n",
    "    cols = {}\n",
    "    for hidx in range(len(histories)):\n",
    "        hist = pd.DataFrame.from_dict(histories[hidx].history)\n",
    "        lab = hist_labels[hidx]\n",
    "        for m in metrics:\n",
    "            prefix = f\"{lab}_{m}\"\n",
    "            col = hist[m]\n",
    "            cols[prefix] = col\n",
    "\n",
    "    dataframe = pd.DataFrame.from_dict(cols)\n",
    "    sns.lineplot(data=dataframe)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"metric\")\n",
    "    if not draw_legend:\n",
    "        plt.legend().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3969f8",
   "metadata": {},
   "source": [
    "# Your own Attention Layer\n",
    "\n",
    "In this section, we will write our own Attention layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f8467a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from keras import ops\n",
    "\n",
    "\n",
    "class SelfAttention(keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self, in_channels, out_channels, key_channels, data_format=\"channels_last\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # we want to establish queries Q, keys K and values V\n",
    "        # instead of using Linear layers, we opt for Conv1D as they use less\n",
    "        # parameters\n",
    "        self.conv_Q = keras.layers.Conv1D(\n",
    "            filters=key_channels, kernel_size=1, data_format=data_format, use_bias=False\n",
    "        )\n",
    "        self.conv_K = keras.layers.Conv1D(\n",
    "            filters=key_channels, kernel_size=1, data_format=data_format, use_bias=False\n",
    "        )\n",
    "        self.conv_V = keras.layers.Conv1D(\n",
    "            filters=out_channels, kernel_size=1, data_format=data_format, use_bias=False\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # run the convolutions on our inputs\n",
    "        Q = self.conv_Q(inputs)\n",
    "        K = self.conv_K(inputs)\n",
    "        V = self.conv_V(inputs)\n",
    "\n",
    "        # You will need to use the Keras OPS API for the operations below\n",
    "        # https://keras.io/api/ops/\n",
    "\n",
    "        # TODO: perform a tensor transpose\n",
    "        #       you want to transpose the very last dimension with the second to last\n",
    "...\n",
    "\n",
    "        # TODO: perform a matrix multiplication of Q*K_t\n",
    "...\n",
    "\n",
    "        # TODO: perform a row-wise softmax of A_\n",
    "...\n",
    "\n",
    "        # TODO: perform a matrix multiplication of A*V\n",
    "...\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2e90c1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def custom_attn(inshape=x.shape[-2:], channels=64, ksize=5):\n",
    "    \"a fully convolutional network (fcn) to regress the signal using selfattention\"\n",
    "\n",
    "    inputs = keras.layers.Input(shape=inshape)\n",
    "    x = keras.layers.Conv1D(\n",
    "        channels, ksize, strides=1, padding=\"same\", activation=\"relu\"\n",
    "    )(inputs)\n",
    "    x = keras.layers.Conv1D(\n",
    "        channels, ksize, strides=1, padding=\"same\", activation=\"relu\"\n",
    "    )(x)\n",
    "    # TODO: Use the SelfAttention class that we wrote above\n",
    "...\n",
    "    x = keras.layers.Conv1D(\n",
    "        channels, ksize, strides=1, padding=\"same\", activation=\"relu\"\n",
    "    )(x)\n",
    "    outputs = keras.layers.Conv1D(1, ksize, strides=1, padding=\"same\")(x)\n",
    "\n",
    "    return keras.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"fcn-regression-custom-attention\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3737a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cmodel = custom_attn(x.shape[1:])\n",
    "cmodel.summary()  # a simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93adfc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmodel.compile(optimizer=\"adam\", loss=keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90532230",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "chistory = cmodel.fit(\n",
    "    x, y, validation_data=(x_test, y_test), batch_size=128, epochs=10, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c535fe",
   "metadata": {},
   "source": [
    "## Create a model with attention\n",
    "\n",
    "The idea of attention was published in 2014 by A. Graves in \"Neural Turing Machines\", see https://arxiv.org/abs/1410.5401\n",
    "It was picked up again in 2017 by A. Vaswani et al in \"Attention is all you need\", see https://arxiv.org/abs/1706.03762\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45c72fd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# We reuse the model idea from above\n",
    "def create_attn(inshape=x.shape[-2:], channels=64, ksize=5):\n",
    "    \"a fully convolutional network (fcn) to regress the signal using selfattention from keras\"\n",
    "\n",
    "    inputs = keras.layers.Input(shape=inshape)\n",
    "    x = keras.layers.Conv1D(\n",
    "        channels, ksize, strides=1, padding=\"same\", activation=\"relu\"\n",
    "    )(inputs)\n",
    "    x = keras.layers.Conv1D(\n",
    "        channels, ksize, strides=1, padding=\"same\", activation=\"relu\"\n",
    "    )(x)\n",
    "    # TODO: Keras also has a built-in Attention Layer, find it\n",
    "    #       and use in in similar fashion as our own custom attention above\n",
    "    #       (note, we want to use one attention head)\n",
    "...\n",
    "...\n",
    "...\n",
    "...\n",
    "...\n",
    "    x = keras.layers.Conv1D(\n",
    "        channels, ksize, strides=1, padding=\"same\", activation=\"relu\"\n",
    "    )(x)\n",
    "    outputs = keras.layers.Conv1D(1, ksize, strides=1, padding=\"same\")(x)\n",
    "\n",
    "    return keras.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"fcn-regression-selfattention\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae96837",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "amodel = create_attn(x.shape[1:])\n",
    "amodel.summary()  # a simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774c081e",
   "metadata": {},
   "source": [
    "The keras built-in attention layer uses Linear layers internally. This gives rise to the large number of parameters in the multi-head attention layer above even though we only want to use 1 head.\n",
    "\n",
    "Let's compile the model and see the effect of this change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1afdfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "amodel.compile(optimizer=\"adam\", loss=keras.losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4b28b0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "ahistory = amodel.fit(\n",
    "    x, y, validation_data=(x_test, y_test), batch_size=128, epochs=15, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f159ce65",
   "metadata": {},
   "source": [
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f08167",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histories(\n",
    "    [history, ahistory, chistory],\n",
    "    [\"loss\", \"val_loss\"],\n",
    "    \"vanilla,self-attention, custom-attention\".split(\",\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "python",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (ipykernel)",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "debugger": true
   },
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
