{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6601f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This python file contains helper classes and functions which make the latter notebooks\n",
    "# more readable.\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7947de15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from mnist1d.data import get_dataset_args, make_dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba963619",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST1D(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 train:bool = True,\n",
    "                 test_size: float = 0.1,\n",
    "                 mnist1d_args: dict = get_dataset_args(),\n",
    "                 seed: int = 42):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_training = train\n",
    "        self.data = make_dataset(mnist1d_args)\n",
    "\n",
    "        # dataset split, the same can be done with torch.utils.data.random_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.data['x'],\n",
    "                                                            self.data['y'],\n",
    "                                                            test_size=test_size,\n",
    "                                                            random_state=seed)\n",
    "\n",
    "        # normalize the data\n",
    "        self.X_loc = np.min(X_train)\n",
    "        self.X_scale = np.max(X_train) - np.min(X_test)\n",
    "\n",
    "        # decide training and testing\n",
    "        if train:\n",
    "            self.X = (X_train - self.X_loc)/self.X_scale\n",
    "            self.y = y_train\n",
    "        else:\n",
    "            # use the same normalisation strategy as during training\n",
    "            self.X = (X_test - self.X_loc)/self.X_scale\n",
    "            self.y = y_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        X = torch.from_numpy(self.X[index:index+1, ...].astype(np.float32))\n",
    "        y = torch.from_numpy(self.y[index, ...].astype(np.int64))\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb946a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(torch_model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    function to calculate the number of trainable parameters of a torch.nn.Module\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    torch_model : torch.nn.Module\n",
    "        torch model to calculate number of parameters for\n",
    "\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    > testmodel = torch.nn.Conv1d(in_channels=1, out_channels=1,kernel_size=3, padding=1, bias=False)\n",
    "    > count_params(testmodel)\n",
    "    3\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    value = sum([p.view(-1).shape[0] for p in torch_model.parameters()])\n",
    "\n",
    "    return value"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
